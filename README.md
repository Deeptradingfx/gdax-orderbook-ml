# gdax-orderbook-ml

Application of machine learning to the GDAX orderbook using a stacked bidirectional LSTM/GRU model to predict new support and resistance on a 15-minute basis; Currently under heavy development. 

## Project/File Structure

Latest notebook file(s) with project code:

**8_program_structure_improvement.ipynb**:
    - Even further refinement to program structure
        + Function scope and structure
        + Parsing of raw data into 4 seperate l2 update (4 consecutive 15 minute l2update segements)
    - Machine learning model refinement & training
    
**6_raw_dataset_update.ipynb**:
    - Update to raw_data (raw_data scrape in both MongoDB and csv format, 1 hour of websocket data from GDAX)
    - L2 Snapshot + L2 Updates without overhead of Match data response (does not have Match data unlike the test data, which does have Match data)
        
### Folder/Repository Structure:  

- 'model' folder:
    - Contains .json and .h5 files for Tensorflow/Keras models (trained model and model weight export/import)

- 'documentation' folder: 
    - Presentation for project
    - Previous README file

- 'saved_charts' folder:
    - Output of generate_chart() for candlestick chart with visualized autogenerated support and resistance from autoSR()
    - Screenshot of model layer structure in text format
    - Graphviz output of model layer structure

- 'test_data' folder: 
    - Only has 10 minutes of scraped data for testing, development, and model input prototyping (snapshot + l2 response updates)

- 'raw_data' folder: 
    - 1 hour of scraped data (snapshot + l2 response updates)

- 'design_mockup' folder: 
    - Contains diagrams, drawings, and notes used in the process of model and project design during prototyping, testing, and expansion.

- 'archived_ipynb' folder: 
    - Contains previous Jupyter Notebook files used in the construction, design, and prototyping of components of this project.
        - Jupyter Notebook (.ipynb) notebook files 1-5 & 7
        - Each sucessive notebook was used to construct and test whether at each "stage" if a project of this kind of scope would even be technically possible. 
    - Successive numbered notebooks generally improve and are iterative in nature on previous notebook files for this project.*

### **Publications, whitepapers, and other resources referenced for model structure layout & design:**

- [How to Construct Deep Recurrent Neural Networks](https://arxiv.org/abs/1312.6026)
- [Training and Analysing Deep Recurrent Neural Networks](https://papers.nips.cc/paper/5166-training-and-analysing-deep-recurrent-neural-networks)
- [Where to Apply Dropout in Recurrent Neural Networks for Handwriting Recognition?](https://pdfs.semanticscholar.org/3061/db5aab0b3f6070ea0f19f8e76470e44aefa5.pdf)
- [Dropout improves Recurrent Neural Networks for Handwriting Recognition](https://arxiv.org/pdf/1312.4569.pdf)
- [Speech Recognition with Deep Recurrent Neural Networks](https://arxiv.org/abs/1303.5778)
- [Recurrent Dropout without Memory Loss](https://arxiv.org/abs/1603.05118)

### License 
    - gdax-orderbook-ml: BSD-3 Licensed, Copyright (c) 2018 Timothy Yu
    - gdax-python: MIT Licensed, Copyright (c) 2017 Daniel Paquin 
    - autoSR() function adapted from nakulnayyar/SupResGenerator (https://github.com/nakulnayyar/SupResGenerator)
