{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development of data pipelines from mongodb to ML model\n",
    "\n",
    "- Removal of matplotlib finanace module (deprecated)\n",
    "- Usage of raw_data_pipeline folder for development of data pipeline\n",
    "- Usage of in-line markdown cells in-notebook for readability and consistency\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "from json import loads\n",
    "import datetime \n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import msgpack\n",
    "\n",
    "# Removal of matplotlib finance module imports (deprecated; implement modern replacement)\n",
    "\n",
    "# API-specific imports (local install required; do NOT use default pip install)\n",
    "import gdax\n",
    "    # Python setup.py install with environment activated to install/use\n",
    "    # Install locally with 'python setup.py install' & development branch of gdax-python checked out\n",
    "    # Do not use default gdax pip install package - that version of the package is currently broken\n",
    "        # Default pip install has broken mongo connection and websocket connection close() error\n",
    "        # Development branch of gdax-python has merged pull requests that fix those issues\n",
    "\n",
    "# Pymongo import (connection to local client DB)\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Preprocessing imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from keras.utils import to_categorical \n",
    "\n",
    "# ML imports \n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import Embedding, Flatten\n",
    "from keras.layers import LSTM, GRU\n",
    "from keras.models import load_model\n",
    "from keras.models import model_from_json\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import TimeDistributed\n",
    "\n",
    "# Auto Support and Resistance/autoSR() import requirements\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "pd.core.common.is_list_like = pd.api.types.is_list_like\n",
    "from pandas_datareader import data, wb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import error for Pandas Datareader, cannot import name \"is_list_like\"\n",
    "    # https://github.com/pydata/pandas-datareader/issues/545\n",
    "    # Workaround for pandas 0.23: https://stackoverflow.com/a/50415484\n",
    "        # Before pandas_datareader import:\n",
    "        # pd.core.common.is_list_like = pd.api.types.is_list_like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 9617758536166070181\n",
      "]\n",
      "==============================================\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "###########################################################################    \n",
    "#num_cores = 4\n",
    "#config = tf.ConfigProto(intra_op_parallelism_threads=num_cores,\\\n",
    "        #inter_op_parallelism_threads=num_cores, allow_soft_placement=True,\\\n",
    "        #device_count = {'CPU' : 1, 'GPU' : 1})\n",
    "#session = tf.Session(config=config)\n",
    "#K.set_session(session)\n",
    "###########################################################################\n",
    "\n",
    "### Force Keras/TF to use CPU backend when GPU present by setting device_count (above) to:\n",
    "    # {'CPU' : 1, 'GPU' : 0}\n",
    "\n",
    "# Import to check check for GPU availability for tensorflow backend\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "# Verify GPU availability for tensorflow backend\n",
    "print(device_lib.list_local_devices())\n",
    "print(\"==============================================\")\n",
    "print(K.tensorflow_backend._get_available_gpus())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.23.3'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pandas Version\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Jupyter/Ipython notebook environment variables\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Global variables\n",
    "\n",
    "# Boolean to drop existing mongo collection/scrape upon scrape() init\n",
    "    # Default = False\n",
    "dropFlag = False\n",
    "\n",
    "# Boolean to set size_delta to l2update values for first update to snapshot\n",
    "    # Inital value = False\n",
    "firstUpdate_both = False\n",
    "\n",
    "# Value to track if feature_creation_inital() was run\n",
    "    # Inital value = False\n",
    "inital_feature_run = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import 1 hour of raw data + L2 updates into localized mongodb instance for pipeline development\n",
    "\n",
    "1. Start new mongodb instance in new Terminal\n",
    "2. Copy mongo_raw from raw_data to raw_data_pipeline folder\n",
    "3. Navigate to raw_data_pipeline folder \n",
    "4. import mongo_raw.json into localized mongo instance:\n",
    "`mongoimport --db btcusd_db --collection btcusd_collection --file mongo_raw.json`\n",
    "5. verify db and collection presence with Terminal instance of mongod and/or MongoDB Compass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Connection establishment (MongoDB)\n",
    "    # Requires db and collection 'btcusd_db' and 'btcusd_collection' to be present on localized instance\n",
    "\n",
    "# Establish connection to GDAX public endpoint\n",
    "public_client = gdax.PublicClient()\n",
    "\n",
    "# Mongo database and collection specification:\n",
    "mongo_client = MongoClient('mongodb://localhost:27017/')\n",
    "db = mongo_client.btcusd_db\n",
    "btcusd_collection = db.btcusd_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Intermediate on-disk format for pipeline target: \n",
    "    # HDF5, Paraquet, Feather, Msgpack  \n",
    "import feather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to load and parse data from Mongo into dataframes\n",
    "def load_parse_feather():\n",
    "    \n",
    "    #Collection specification (in database)\n",
    "    input_data = db.btcusd_collection \n",
    "    \n",
    "    # Create individual dataframes for main response types: snapshot, l2update\n",
    "    snapshot = pd.DataFrame(list(input_data.find({'type':'snapshot'})))\n",
    "    l2update = pd.DataFrame(list(input_data.find({'type':'l2update'})))\n",
    "    \n",
    "    ### snapshot/orderbook state response load and parse ###\n",
    "    \n",
    "    # Extract asks/bid individual column of array of arrays into lists\n",
    "    snapshot_asks = snapshot[['asks'][0]][0]\n",
    "    snapshot_bids = snapshot[['bids'][0]][0]\n",
    "    \n",
    "    # Convert list (of array of arrays) into dataframe\n",
    "    snapshot_asks_df =pd.DataFrame(snapshot_asks)\n",
    "    snapshot_bids_df =pd.DataFrame(snapshot_bids)\n",
    "    \n",
    "    # Rename columns to snapshot array format:\n",
    "        # snapshot array format: [price, size]\n",
    "            # [side, price, size] format \n",
    "        # Ask = sell price, bid = buy price\n",
    "    snapshot_asks_df.rename(columns ={0:'price',1:'size'}, inplace =True)\n",
    "    snapshot_bids_df.rename(columns ={0:'price',1:'size'}, inplace =True)\n",
    "    snapshot_asks_df['side'] = \"sell\"\n",
    "    snapshot_bids_df['side'] = \"buy\"\n",
    "    cols =['side','price','size']\n",
    "    snapshot_asks_df = snapshot_asks_df[cols]\n",
    "    snapshot_bids_df = snapshot_bids_df[cols]\n",
    "    \n",
    "    ### L2 update response load and parse ###\n",
    "    \n",
    "    # Restucture l2update to have [side,price,size] from 'changes' column\n",
    "    l2update_clean = l2update[['changes','time']]\n",
    "          \n",
    "    # Convert changes list of lists -> into array \n",
    "    l2_array = np.ravel(l2update_clean['changes']) \n",
    "    # Flatten the list and remove outer bracket:\n",
    "    flattened = [val for sublist in l2_array for val in sublist]\n",
    "        # Reference: https://stackoverflow.com/questions/11264684/flatten-list-of-lists?\n",
    "    # Convert back to dataframe and combine with timestamps from l2update:\n",
    "    changes_df= pd.DataFrame.from_records(flattened)\n",
    "    # Add time column back to L2 update dataframe\n",
    "    l2update_formatted = pd.concat([changes_df,l2update_clean['time']],1)\n",
    "    # Rename columns for [side, price, size]:\n",
    "    l2update_formatted.rename({0:\"side\",1:\"price\",2:\"size\"}, axis ='columns',inplace=True)\n",
    "    \n",
    "    # Save parsed data to csv (API -> Mongo -> Dataframe -> .csv)\n",
    "        # Save data to .csv format in raw_data folder\n",
    "    l2update_formatted.to_feather(\"raw_data_pipeline/l2update.feather\")#,header=True,encoding='utf-8',index =False)\n",
    "    snapshot_asks_df.to_feather(\"raw_data_pipeline/snapshot_asks.feather\")#,header=True,encoding='utf-8',index =False)\n",
    "    snapshot_bids_df.to_feather(\"raw_data_pipeline/snapshot_bids.feather\")#,header=True,encoding='utf-8',index =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_parse_feather()\n",
    "# Axis error on rename, Pandas upgrade required:\n",
    "    # https://stackoverflow.com/questions/47800034/pandas-dataframe-rename-unexpected-keyword-argument-axis-when-using-mapper/47800303"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
