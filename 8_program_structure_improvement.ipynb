{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "from json import loads\n",
    "import datetime \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import time\n",
    "\n",
    "# Charting-specific imports (matplotlib, matplotlib.finance)\n",
    "import matplotlib.finance\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.finance import candlestick_ohlc,candlestick2_ohlc\n",
    "#from matplotlib.finance import volume_overlay, volume_overlay2\n",
    "from matplotlib.dates import  DateFormatter, epoch2num\n",
    "    # https://matplotlib.org/api/finance_api.html#module-matplotlib.finance\n",
    "import matplotlib.patheffects as PathEffects\n",
    "    # [plt.text object].set_path_effects([PathEffects.withStroke(linewidth=5, foreground='w')])\n",
    "    # plt.text reference: https://matplotlib.org/api/text_api.html#matplotlib.text.Text\n",
    "\n",
    "# API-specific imports (local install required; do NOT use default pip install)\n",
    "import gdax\n",
    "    # Python setup.py install with environment activated to install/use\n",
    "    # Install locally with 'python setup.py install' & development branch of gdax-python checked out\n",
    "    # Do not use default gdax pip install package - that version of the package is currently broken\n",
    "        # Default pip install has broken mongo connection and websocket connection close() error\n",
    "\n",
    "# Pymongo import for connection to local client DB\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Preprocessing Imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from keras.utils import to_categorical \n",
    "\n",
    "# ML Imports \n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import Embedding, Flatten\n",
    "from keras.layers import LSTM, GRU\n",
    "from keras.models import load_model\n",
    "from keras.models import model_from_json\n",
    "from keras import backend as K\n",
    "\n",
    "# autoSR() function import requirements\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "from pandas_datareader import data, wb\n",
    "\n",
    "###########################################################################\n",
    "### Force Keras/TF to use CPU backend when GPU present by setting:\n",
    "    # {'CPU' : 1, 'GPU' : 0}\n",
    "    \n",
    "#num_cores = 4\n",
    "#config = tf.ConfigProto(intra_op_parallelism_threads=num_cores,\\\n",
    "        #inter_op_parallelism_threads=num_cores, allow_soft_placement=True,\\\n",
    "        #device_count = {'CPU' : 1, 'GPU' : 1})\n",
    "#session = tf.Session(config=config)\n",
    "#K.set_session(session)\n",
    "###########################################################################\n",
    "\n",
    "# Import to check check for GPU availability for tensorflow backend\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "# Verify GPU availability for tensorflow backend\n",
    "print(device_lib.list_local_devices())\n",
    "print(\"==============================================\")\n",
    "print(K.tensorflow_backend._get_available_gpus())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "\n",
    "# Boolean to drop existing mongo collection/scrape upon scrape() init\n",
    "dropFlag = False\n",
    "\n",
    "# Boolean to set size_delta to l2update values for first update to snapshot\n",
    "firstUpdate_bids = False\n",
    "firstUpdate_asks = False\n",
    "\n",
    "# Value to track if feature_creation_inital() was run\n",
    "    # Inital value = False\n",
    "inital_feature_run = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection establishment\n",
    "\n",
    "# Establish connection to GDAX public endpoint\n",
    "public_client = gdax.PublicClient()\n",
    "\n",
    "# Mongo database and collection specification:\n",
    "mongo_client = MongoClient('mongodb://localhost:27017/')\n",
    "db = mongo_client.btcusd_db\n",
    "btcusd_collection = db.btcusd_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to start scrape process from websocket to mongodb instance\n",
    "def scrape_start():\n",
    "    \n",
    "    # Drop existing collection from db if dropFlag == True (on new scrape):\n",
    "    if 'btcusd_db' in mongo_client.database_names() and dropFlag is True:\n",
    "        mongo_client['btcusd_db'].drop_collection('btcusd_collection')\n",
    "        #print(mongo_client.database_names())\n",
    "        #print(db.collection_names())\n",
    "        \n",
    "    # Start instance of websocket client for L2 Orderbook + L2 update data request and scrape\n",
    "    wsClient = gdax.WebsocketClient(url=\"wss://ws-feed.gdax.com\", \n",
    "                                products=[\"BTC-USD\"],\\\n",
    "                                message_type=\"subscribe\",\\\n",
    "                                channels =[\"level2\"],\\\n",
    "                                mongo_collection=btcusd_collection,\\\n",
    "                                should_print=False)\n",
    "    \n",
    "    # Save request open time and start websocket\n",
    "    time.sleep(4)\n",
    "    request_time_start=public_client.get_time()\n",
    "    wsClient.start()\n",
    "    \n",
    "    # scrape_time is variable for time between websocket connection start and end\n",
    "        # Defined in seconds\n",
    "        # i.e. 600 seconds = scrape running for 10 minutes\n",
    "    scrape_time = 600\n",
    "\n",
    "    time.sleep(scrape_time)\n",
    "    # Save request close time and close websocket\n",
    "    request_time_end=public_client.get_time()\n",
    "    wsClient.close()\n",
    "    \n",
    "    # Append request times for open/close of websocket stream to dataframe, save to csv\n",
    "    request_log_df = pd.DataFrame.from_dict({'request start':request_time_start,'request end':request_time_end},orient ='index')\n",
    "    request_log_df.to_csv(\"raw_data/request_log.csv\",header=True,encoding='utf-8',index =True)\n",
    "\n",
    "# Function to load and parse data from Mongo into dataframes\n",
    "def load_parse():\n",
    "    \n",
    "    #Collection specification (in database)\n",
    "    input_data = db.btcusd_collection \n",
    "    \n",
    "    # Create individual dataframes for main response types: snapshot, l2update\n",
    "    snapshot = pd.DataFrame(list(input_data.find({'type':'snapshot'})))\n",
    "    l2update = pd.DataFrame(list(input_data.find({'type':'l2update'})))\n",
    "    \n",
    "    ### snapshot  response load and parse ###\n",
    "    \n",
    "    # Extract asks/bid individual column of array of arrays into lists\n",
    "    snapshot_asks = snapshot[['asks'][0]][0]\n",
    "    snapshot_bids = snapshot[['bids'][0]][0]\n",
    "    \n",
    "    # Convert list (of array of arrays) into dataframe\n",
    "    snapshot_asks_df =pd.DataFrame(snapshot_asks)\n",
    "    snapshot_bids_df =pd.DataFrame(snapshot_bids)\n",
    "    \n",
    "    # Rename columns to snapshot array format:\n",
    "        # snapshot array format: [price, size]\n",
    "            # [side, price, size] format \n",
    "        # Ask = sell price, bid = buy price\n",
    "    snapshot_asks_df.rename(columns ={0:'price',1:'size'}, inplace =True)\n",
    "    snapshot_bids_df.rename(columns ={0:'price',1:'size'}, inplace =True)\n",
    "    snapshot_asks_df['side'] = \"sell\"\n",
    "    snapshot_bids_df['side'] = \"buy\"\n",
    "    cols =['side','price','size']\n",
    "    snapshot_asks_df = snapshot_asks_df[cols]\n",
    "    snapshot_bids_df = snapshot_bids_df[cols]\n",
    "    \n",
    "    ### L2 update response load and parse ###\n",
    "    \n",
    "    # Restucture l2update to have [side,price,size] from 'changes' column\n",
    "    l2update_clean = l2update[['changes','time']]\n",
    "          \n",
    "    # Convert changes list of lists -> into array \n",
    "    l2_array = np.ravel(l2update_clean['changes']) \n",
    "    # Flatten the list and remove outer bracket:\n",
    "    flattened = [val for sublist in l2_array for val in sublist]\n",
    "        # Reference: https://stackoverflow.com/questions/11264684/flatten-list-of-lists?\n",
    "    # Convert back to dataframe and combine with timestamps from l2update:\n",
    "    changes_df= pd.DataFrame.from_records(flattened)\n",
    "    # Add time column back to L2 update dataframe\n",
    "    l2update_formatted = pd.concat([changes_df,l2update_clean['time']],1)\n",
    "    # Rename columns for [side, price, size]:\n",
    "    l2update_formatted.rename({0:\"side\",1:\"price\",2:\"size\"}, axis ='columns',inplace=True)\n",
    "    \n",
    "    # Save parsed data to csv (API -> Mongo -> Dataframe -> .csv)\n",
    "    #save_csv()\n",
    "    \n",
    "    # Save data to .csv format in raw_data folder\n",
    "    l2update_formatted.to_csv(\"raw_data/l2update.csv\",header=True,encoding='utf-8',index =False)\n",
    "    snapshot_asks_df.to_csv(\"raw_data/snapshot_asks.csv\",header=True,encoding='utf-8',index =False)\n",
    "    snapshot_bids_df.to_csv(\"raw_data/snapshot_bids.csv\",header=True,encoding='utf-8',index =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in raw data from csv, 'raw_data' folder\n",
    "    # 'raw_data' folder has 1hr of scraped data (snapshot + l2 response updates)\n",
    "    # 'test_data' folder only has 10 minutes of scraped data \n",
    "def raw_csv_load():\n",
    "    global request_log_df\n",
    "    global snapshot_asks_df\n",
    "    global snapshot_bids_df\n",
    "    global l2update_df\n",
    "  \n",
    "    snapshot_asks_df = pd.read_csv(\"raw_data/snapshot_asks.csv\",dtype ={'size':float,'size_delta':float})\n",
    "    snapshot_bids_df = pd.read_csv(\"raw_data/snapshot_bids.csv\",dtype ={'size':float,'size_delta':float})\n",
    "    l2update_df = pd.read_csv(\"raw_data/l2update.csv\", dtype ={'size':float})\n",
    "    request_log_df= pd.read_csv(\"raw_data/request_log.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Request 15minute chart data #####\n",
    "\n",
    "def chart_15m_request(request_log_df):\n",
    "    \n",
    "    global chart_15m_df\n",
    "    request_start = request_log_df['iso'][1]\n",
    "    request_end = request_log_df['iso'][0]\n",
    "\n",
    "    # Request 15 minutes of candlestick (open high low close) data from API\n",
    "        # start time must be in ISO 8601 format for get_product_historic_rates()\n",
    "    chart_15m = public_client.get_product_historic_rates('BTC-USD', start = request_start, granularity=60)\n",
    "\n",
    "    # Convert chart data response into dataframes\n",
    "    chart_15m_df =pd.DataFrame.from_records(chart_15m,columns=[\"time\",\"low\", \"high\",\"open\" ,\"close\", \"volume\"])\n",
    "\n",
    "    # Reorder columns according to label order required by matplotlib finance package\n",
    "    chart_15m_df = chart_15m_df[[\"time\",\"open\",\"high\",\"low\",\"close\",\"volume\"]]\n",
    "\n",
    "    # Reverse/sort timestamp order (without reversal chart labels/axis will be out of order)\n",
    "    chart_15m_df.sort_values(by='time',axis=0, inplace =True)\n",
    "\n",
    "    #Convert/sort time to datetime object (for matplotlib chart format requirement)\n",
    "    chart_15m_df['time'] =pd.to_datetime(chart_15m_df['time'],unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto support/resistance adapted into function \n",
    "    # Using estimate_bandwidth and meanshift() from sklearn.cluster library\n",
    "    # Source: Adapted from https://github.com/nakulnayyar/SupResGenerator\n",
    "    # Takes dataframe object with open/high/low/close values \n",
    "        # Returns list of auto-generated support and resistance levels for price action\n",
    "def autoSR(dataframe):\n",
    "    \n",
    "    data = chart_15m_df.as_matrix(columns=['close'])\n",
    "    data2 = data[:len(data)*1]\n",
    "    bandwidth = estimate_bandwidth(data2, quantile=0.1, n_samples=100)\n",
    "    ms = MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "    ms.fit(data2)\n",
    "    ml_results = []\n",
    "    for k in range(len(np.unique(ms.labels_))):\n",
    "            my_members = ms.labels_ == k\n",
    "            values = data[my_members, 0]    \n",
    "            ml_results.append(min(values))\n",
    "            ml_results.append(max(values))\n",
    "            # Remove duplicate S/R level values\n",
    "            # Using sets\n",
    "            ml_set =set(ml_results)\n",
    "            ml_results = list(ml_set)\n",
    "            # Sort values before return output \n",
    "            ml_results.sort()\n",
    "    \n",
    "    # Convert ml_results into sorted int array\n",
    "    ml_results_modified =np.asarray(ml_results)\n",
    "    ml_results_modified =np.trunc(ml_results_modified).astype(int)\n",
    "    ml_results_modified = np.sort(ml_results_modified,kind = 'quicksort') \n",
    "    # Remove duplicates from int conversion\n",
    "    ml_results_modified_set=set(ml_results_modified)\n",
    "    ml_results_modified = list(ml_results_modified_set)\n",
    "    ml_results_modified = np.sort(ml_results_modified,kind = 'quicksort')\n",
    "    ml_results_modified\n",
    "            \n",
    "    return ml_results_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_chart(dataframe):\n",
    "\n",
    "    matplotlib.rcParams.update({'font.size': 14})\n",
    "    dataframe = chart_15m_df \n",
    "    \n",
    "    # Generate chart of past 15 minutes with autogenerated support and resistance levels:\n",
    "    fig, ax = plt.subplots(figsize=(20, 14))\n",
    "    candlestick2_ohlc(ax,chart_15m_df['open'],chart_15m_df['high'],\\\n",
    "                      chart_15m_df['low'],chart_15m_df['close'],width=2,\\\n",
    "                      colorup='k',colordown='r',alpha=.5)\n",
    "    ax.set_xticklabels(chart_15m_df['time'] ,rotation=30)\n",
    "    \n",
    "    ax.xaxis.set_major_locator(ticker.MaxNLocator(17))\n",
    "        # n + 2 for proper label set at 20,18/20,14 chart size\n",
    "    plt.yticks(np.arange(int(min(chart_15m_df['low'])-10), max(chart_15m_df['high'])+10, 10))\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Price (USD)')\n",
    "    #ax.minorticks_on()\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.tick_params(axis='y', pad=10)\n",
    "    ax.grid(which='major', linestyle='--', linewidth='0.2', color='b')\n",
    "    matplotlib.pyplot.title(\"15 Min BTC/USD with Support/Resistance Levels\")\n",
    "    \n",
    "    ml_results_modified = autoSR(chart_15m_df)\n",
    "    \n",
    "    # Add S/R labels to chart\n",
    "        # count increment is used for staggering of labels in conjunction with modulus\n",
    "        \n",
    "    count = 1\n",
    "    for k in ml_results_modified:\n",
    "        \n",
    "        #count = count+ 2.12\n",
    "        ax.axhline(y=k)\n",
    "       \n",
    "        if count%3 == 0:\n",
    "            plt.text(y=k,s=k,x=count-5,color='blue',rotation=45,size ='large').set_path_effects([PathEffects.withStroke(linewidth=5, foreground='w')])\n",
    "        if count%3 == 1:\n",
    "            plt.text(y=k,s=k,x=count+5,color='blue',rotation=45,size ='large').set_path_effects([PathEffects.withStroke(linewidth=5, foreground='w')])\n",
    "        if count%3 == 2:\n",
    "            plt.text(y=k,s=k,x=count+16,color='blue',rotation=45, size ='large').set_path_effects([PathEffects.withStroke(linewidth=5, foreground='w')])\n",
    "        \n",
    "        \n",
    "        count = count +1 \n",
    "    #plt.tight_layout()\n",
    "    return plt.show()\n",
    "    #return plt.show(), ml_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and set inital/placeholder values for size_delta, position, and sr_prox_value/line input features \n",
    "    # run once upon new scrape only\n",
    "def feature_creation_inital():\n",
    "    global inital_feature_run\n",
    "    \n",
    "    if inital_feature_run == False:\n",
    "           \n",
    "        # Set inital size delta l2 snapshot - use decimal point to force as float\n",
    "        snapshot_asks_df['size_delta'] = 0.0\n",
    "        snapshot_bids_df['size_delta'] = 0.0\n",
    "\n",
    "        # Set inital position for l2 snapshot\n",
    "        snapshot_asks_df['position'] = snapshot_asks_df.index +1\n",
    "        snapshot_bids_df['position'] = snapshot_bids_df.index +1\n",
    "\n",
    "        # Reverse index for sell/asks\n",
    "        # snapshot_asks_df =snapshot_asks_df[::-1]\n",
    "        # Reverse inital position indicator for bids/buy side\n",
    "        rev = snapshot_bids_df['position'].values * -1\n",
    "        snapshot_bids_df['position'] = rev\n",
    "\n",
    "        # Set inital proximity value and inital value for nearest prox line\n",
    "        snapshot_asks_df['sr_prox_value'] = 0\n",
    "        snapshot_asks_df['sr_prox_line'] = 0\n",
    "        snapshot_bids_df['sr_prox_value'] = 0\n",
    "        snapshot_bids_df['sr_prox_line'] = 0\n",
    "\n",
    "        # Set feature_creation_inital_run flag to True once run\n",
    "        feature_creation_inital_run = True\n",
    "        print(\"run\")   \n",
    "    elif inital_feature_run == True:\n",
    "        # Required syntax for logical operator/ if-else structure, but does nothing as a result\n",
    "        # i.e., a \"pass\"\n",
    "        print(\"pass\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "# Function execution order/testing \n",
    "\n",
    "raw_csv_load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_log_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_15m_request(request_log_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart_15m_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoSR(chart_15m_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_chart(chart_15m_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_creation_inital()\n",
    "snapshot_asks_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
