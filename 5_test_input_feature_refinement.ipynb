{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "from json import loads\n",
    "import datetime \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import time\n",
    "\n",
    "# Charting-specific imports\n",
    "import matplotlib.finance\n",
    "import matplotlib.ticker as mticker\n",
    "from matplotlib.finance import candlestick_ohlc,candlestick2_ohlc\n",
    "from matplotlib.finance import volume_overlay,volume_overlay2\n",
    "from matplotlib.dates import  DateFormatter, epoch2num\n",
    "    # https://matplotlib.org/api/finance_api.html#module-matplotlib.finance\n",
    "\n",
    "# API-specific imports (local install)\n",
    "import gdax\n",
    "    # Python setup.py install with environment activaated to install/use\n",
    "    # Do not use default gdax pip install package - that version of the package is currently broken\n",
    "\n",
    "# Pymongo import for connection to local client DB\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Preprocessing Imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from keras.utils import to_categorical \n",
    "\n",
    "# ML Imports \n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM, GRU\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "###########################################################################\n",
    "### Force Keras/TF to use CPU backend when GPU present by setting:\n",
    "    # {'CPU' : 1, 'GPU' : 0}\n",
    "    \n",
    "#num_cores = 4\n",
    "#config = tf.ConfigProto(intra_op_parallelism_threads=num_cores,\\\n",
    "        #inter_op_parallelism_threads=num_cores, allow_soft_placement=True,\\\n",
    "        #device_count = {'CPU' : 1, 'GPU' : 1})\n",
    "#session = tf.Session(config=config)\n",
    "#K.set_session(session)\n",
    "###########################################################################\n",
    "\n",
    "# Import to check check for GPU availability for tensorflow backend\n",
    "from tensorflow.python.client import device_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 4395846153625972558\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 9222031934\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 8986549787046924253\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n",
      "==============================================\n",
      "['/job:localhost/replica:0/task:0/device:GPU:0']\n"
     ]
    }
   ],
   "source": [
    "# Verify GPU availability for tensorflow backend\n",
    "print(device_lib.list_local_devices())\n",
    "print(\"==============================================\")\n",
    "print(K.tensorflow_backend._get_available_gpus())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-17T00:55:03.354Z\n",
      "2018-04-17T01:05:03.469Z\n"
     ]
    }
   ],
   "source": [
    "# Request log load\n",
    "request_log_df= pd.read_csv(\"test_data/request_log.csv\",dtype ={'epoch':object})\n",
    "request_start = request_log_df['iso'][1]\n",
    "request_end = request_log_df['iso'][0]\n",
    "print(request_start)\n",
    "print(request_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "\n",
    "# Boolean to drop existing mongo collection/scrape upon scrape() init\n",
    "dropFlag = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection establishment\n",
    "\n",
    "# Establish connection to GDAX public endpoint\n",
    "public_client = gdax.PublicClient()\n",
    "\n",
    "# Mongo database and collection specification:\n",
    "mongo_client = MongoClient('mongodb://localhost:27017/')\n",
    "db = mongo_client.btcusd_db\n",
    "btcusd_collection = db.btcusd_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to start scrape process from websocket to mongodb instance\n",
    "def scrape_start():\n",
    "    \n",
    "    # Drop existing collection from db if dropFlag == True (on new scrape):\n",
    "    if 'btcusd_db' in mongo_client.database_names() and dropFlag is True:\n",
    "        db['btcusd_db'].drop_collection('btcusd_collection')\n",
    "        #print(mongo_client.database_names())\n",
    "        #print(db.collection_names())\n",
    "        \n",
    "    # Start instance of websocket client for L2 Orderbook + L2 update data request and scrape\n",
    "    wsClient = gdax.WebsocketClient(url=\"wss://ws-feed.gdax.com\", \n",
    "                                products=[\"BTC-USD\"],\\\n",
    "                                message_type=\"subscribe\",\\\n",
    "                                channels =[\"level2\"],\\\n",
    "                                mongo_collection=btcusd_collection,\\\n",
    "                                should_print=False)\n",
    "    \n",
    "    # Save request open time and start websocket\n",
    "    time.sleep(4)\n",
    "    request_time_start=public_client.get_time()\n",
    "    wsClient.start()\n",
    "    \n",
    "    # scrape_time is variable for time between websocket connection start and end\n",
    "        # Defined in seconds\n",
    "        # i.e. 600 seconds = scrape running for 10 minutes\n",
    "    scrape_time = 600\n",
    "\n",
    "    time.sleep(scrape_time)\n",
    "    # Save request close time and close websocket\n",
    "    request_time_end=public_client.get_time()\n",
    "    wsClient.close()\n",
    "    \n",
    "    # Append request times for open/close of websocket stream to dataframe, save to csv\n",
    "    request_log_df = pd.DataFrame.from_dict({'request start':request_time_start,'request end':request_time_end},orient ='index')\n",
    "    request_log_df.to_csv(\"raw_data/request_log.csv\",header=True,encoding='utf-8',index =True)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and parse data from Mongo into dataframes\n",
    "def load_parse():\n",
    "    \n",
    "    #Collection specification (in database)\n",
    "    input_data = db.btcusd_collection \n",
    "    \n",
    "    # Create individual dataframes for main response types: snapshot, l2update\n",
    "    snapshot = pd.DataFrame(list(input_data.find({'type':'snapshot'})))\n",
    "    l2update = pd.DataFrame(list(input_data.find({'type':'l2update'})))\n",
    "    \n",
    "    ### snapshot  response load and parse ###\n",
    "    \n",
    "    # Extract asks/bid individual column of array of arrays into lists\n",
    "    snapshot_asks = snapshot[['asks'][0]][0]\n",
    "    snapshot_bids = snapshot[['bids'][0]][0]\n",
    "    \n",
    "    # Convert list (of array of arrays) into dataframe\n",
    "    snapshot_asks_df =pd.DataFrame(snapshot_asks)\n",
    "    snapshot_bids_df =pd.DataFrame(snapshot_bids)\n",
    "    \n",
    "    # Rename columns to snapshot array format:\n",
    "        # snapshot array format: [price, size]\n",
    "            # [side, price, size] format for one-hot encoding\n",
    "        # Ask = sell price, bid = buy price\n",
    "    snapshot_asks_df.rename(columns ={0:'price',1:'size'}, inplace =True)\n",
    "    snapshot_bids_df.rename(columns ={0:'price',1:'size'}, inplace =True)\n",
    "    snapshot_asks_df['side'] = \"sell\"\n",
    "    snapshot_bids_df['side'] = \"buy\"\n",
    "    cols =['side','price','size']\n",
    "    snapshot_asks_df = snapshot_asks_df[cols]\n",
    "    snapshot_bids_df = snapshot_bids_df[cols]\n",
    "    \n",
    "    ### L2 update response load and parse ###\n",
    "    \n",
    "    # Restucture l2update to have [side,price,size] from 'changes' column\n",
    "    l2update_clean = l2update[['changes','time']]\n",
    "          \n",
    "    # Convert changes list of lists -> into array \n",
    "    l2_array = np.ravel(l2update_clean['changes']) \n",
    "    # Flatten the list and remove outer bracket:\n",
    "    flattened = [val for sublist in l2_array for val in sublist]\n",
    "        # Reference: https://stackoverflow.com/questions/11264684/flatten-list-of-lists?\n",
    "    # Convert back to dataframe and combine with timestamps from l2update:\n",
    "    changes_df= pd.DataFrame.from_records(flattened)\n",
    "    # Add time column back to L2 update dataframe\n",
    "    l2update_formatted = pd.concat([changes_df,l2update_clean['time']],1)\n",
    "    # Rename columns for [side, price, size]:\n",
    "    l2update_formatted.rename({0:\"side\",1:\"price\",2:\"size\"}, axis ='columns',inplace=True)\n",
    "    \n",
    "    # Save parsed data to csv (API -> Mongo -> Dataframe -> .csv)\n",
    "    save_csv()\n",
    "    \n",
    "def save_csv():\n",
    "    \n",
    "    # Save data to .csv format in raw_data folder\n",
    "    l2update_formatted.to_csv(\"raw_data/l2update.csv\",header=True,encoding='utf-8',index =False)\n",
    "    snapshot_asks_df.to_csv(\"raw_data/snapshot_asks.csv\",header=True,encoding='utf-8',index =False)\n",
    "    snapshot_bids_df.to_csv(\"raw_data/snapshot_bids.csv\",header=True,encoding='utf-8',index =False)\n",
    "    subscriptions.to_csv(\"raw_data/subscriptions.csv\",header=True,encoding='utf-8',index =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read in data from csv into dataframes\n",
    "def data_read():\n",
    "    \n",
    "    # Read in test data for L2 orderbook state (bids + asks)\n",
    "    # Read in test data for subsquent L2 orderbook update states (L2 updates to bid + asks)\n",
    "    snapshot_asks_df = pd.read_csv(\"test_data/snapshot_asks.csv\")\n",
    "    snapshot_bids_df = pd.read_csv(\"test_data/snapshot_bids.csv\")\n",
    "    l2update_df = pd.read_csv(\"test_data/l2update.csv\", dtype ={'changes':object})\n",
    "    request_log_df= pd.read_csv(\"test_data/request_log.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
